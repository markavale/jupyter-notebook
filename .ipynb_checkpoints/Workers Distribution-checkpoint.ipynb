{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf73dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, json, time, random, pandas as pd\n",
    "from pprint import pprint\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b89cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spider(data):\n",
    "    # INITIALIZATION\n",
    "    spiders = []\n",
    "    MAX_THREAD = 2\n",
    "    # CHUNK SPIDER\n",
    "    step_n = math.ceil(len(data) / MAX_THREAD) # Return the ceiling of step_n, the smallest integer greater than or equal to step_n\n",
    "    spider_data = [data[i:i + step_n]\n",
    "                   for i in range(0, len(data), step_n)]\n",
    "    # SPIDER CRAWLER\n",
    "#     print(\"Total thread spider(s): {}\".format(len(spider_data)))\n",
    "    # print(\"Divisible_n:\", divisible_n)\n",
    "    for spider in spider_data:\n",
    "        spiders.append(len(spider))\n",
    "    return spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d1240d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(system_data, WORKERS):\n",
    "    # Computation for divisible number for chunking URL(s)\n",
    "    divisible_n = math.ceil(len(system_data) / WORKERS)\n",
    "    data = [system_data[i:i + divisible_n]\n",
    "            for i in range(0, len(system_data), divisible_n)]  # Chunking URL(s)\n",
    "\n",
    "    # Instansiating MAX WORKERS based on the lower number between length of data and defined WORKERS\n",
    "    MAX_WORKERS = min(len(data), WORKERS)\n",
    "\n",
    "    # LOGGING ---------------------------\n",
    "    print(\"Total URL(s): {}\".format(len(system_data)))\n",
    "    print(\"Total Spider(s) / Worker(s): {}\".format(MAX_WORKERS))\n",
    "    print(\"Chunked Data for multiprocessing: \",\n",
    "          list(map(lambda d: len(d), data)))\n",
    "    # MULTI PROCESSING OF SPIDER FUNCTION\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future = [executor.submit(spider, obj) for obj in data]\n",
    "\n",
    "    # Getting the return value of multi spiders\n",
    "    spiders = [obj.result() for obj in future]\n",
    "    return len(system_data), MAX_WORKERS, spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25abaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(spiders):\n",
    "    rows = []\n",
    "    for i in range(1, len(spiders[0]) + 1):\n",
    "        rows.append(\"Thread {}\".format(i))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234bc6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers(total_workers):\n",
    "    headers = []\n",
    "    for i in range(1, total_workers + 1):\n",
    "        headers.append(\"Worker {}\".format(i))\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e65f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_parser():\n",
    "    \n",
    "    DATA = 500\n",
    "    articles = list(map(lambda x: x.strip(), open(\n",
    "        'test-data/test-articles.txt').read().split('\\n'))) * DATA\n",
    "    if len(articles) == 0:\n",
    "        print(\"No Data\")\n",
    "        exit(0)\n",
    "    # Get the machine's total workers\n",
    "    WORKERS = 14#os.cpu_count() - 2\n",
    "    total_data, total_workers, spiders = main(articles, WORKERS)\n",
    "    headers = get_headers(total_workers)\n",
    "    fields = get_rows(spiders)\n",
    "    data_set = pd.DataFrame(spiders, index = headers, columns = fields)\n",
    "    print(data_set.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "109b5b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URL(s): 500\n",
      "Total Spider(s) / Worker(s): 14\n",
      "Chunked Data for multiprocessing:  [36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 32]\n",
      "          Worker 1  Worker 2  Worker 3  Worker 4  Worker 5  Worker 6  \\\n",
      "Thread 1        18        18        18        18        18        18   \n",
      "Thread 2        18        18        18        18        18        18   \n",
      "\n",
      "          Worker 7  Worker 8  Worker 9  Worker 10  Worker 11  Worker 12  \\\n",
      "Thread 1        18        18        18         18         18         18   \n",
      "Thread 2        18        18        18         18         18         18   \n",
      "\n",
      "          Worker 13  Worker 14  \n",
      "Thread 1         18         16  \n",
      "Thread 2         18         16  \n"
     ]
    }
   ],
   "source": [
    "execute_parser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
